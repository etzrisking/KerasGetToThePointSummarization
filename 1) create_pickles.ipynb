{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm_notebook\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = r'D:\\projects\\nlp\\summarization\\stories'\n",
    "VOCAB_DIR = r'D:\\projects\\nlp\\summarization\\stories\\vocab'\n",
    "SAVE_PKL_DIR = r'D:\\projects\\nlp\\summarization\\stories\\all_pkl'\n",
    "SAVE_BIN_DIR = r'D:\\projects\\nlp\\summarization\\stories\\all_bin'\n",
    "RESTART = True\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_perc_1 = re.compile('[0-9]+[.]?[0-9]+\\s*%')\n",
    "re_curr_1 = re.compile('[0-9]+[.]?[0-9]+\\s*[лв|CHF|Kč|kr|€|£|kn|₾|ft|kr|zł|₽|lei|kr|₺|₴|د.إ|₪|Ksh|.د.م|₦|R|R$|$|$|$|$|S/.|$|$|৳|¥|元|HK$|Rp|₹|¥|RM|$|₱|Rs|$|₩|Rs|฿|₫|₿|₿|XRP|ɱ|Ł|Ξ|€|£|$|¥] ')\n",
    "re_curr_2 = re.compile(' [лв|CHF|Kč|kr|€|£|kn|₾|ft|kr|zł|₽|lei|kr|₺|₴|د.إ|₪|Ksh|.د.م|₦|R|R$|$|$|$|$|S/.|$|$|৳|¥|元|HK$|Rp|₹|¥|RM|$|₱|Rs|$|₩|Rs|฿|₫|₿|₿|XRP|ɱ|Ł|Ξ|€|£|$|¥]\\s*[0-9]+[.]?[0-9]+')\n",
    "re_curr_3 = re.compile('[0-9]+[.]?[0-9]+\\s*[BGN|CHF|CZK|DKK|EUR|GBP|HRK|GEL|HUF|NOK|PLN|RUB|RON|SEK|TRY|UAH|AED|ILS|KES|MAD|NGN|ZAR|BRL|CAD|CLP|COP|MXN|PEN|USD|AUD|BDT|CNY|CNY|HKD|IDR|INR|JPY|MYR|NZD|PHP|PKR|SGD|KRW|LKR|THB|VND|XBT|BTC|XRP|XMR|LTC|ETH|EUR|GBP|USD|JPY] ')\n",
    "re_curr_4 = re.compile(' [BGN|CHF|CZK|DKK|EUR|GBP|HRK|GEL|HUF|NOK|PLN|RUB|RON|SEK|TRY|UAH|AED|ILS|KES|MAD|NGN|ZAR|BRL|CAD|CLP|COP|MXN|PEN|USD|AUD|BDT|CNY|CNY|HKD|IDR|INR|JPY|MYR|NZD|PHP|PKR|SGD|KRW|LKR|THB|VND|XBT|BTC|XRP|XMR|LTC|ETH|EUR|GBP|USD|JPY]\\s*[0-9]+[.]?[0-9]+')\n",
    "re_numb_1 = re.compile('[0-9]+[.]?[0-9]+')\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re_perc_1.sub(' <percent> ', text)\n",
    "    text = re_curr_2.sub(' <currency> ', text)\n",
    "    text = re_curr_1.sub(' <currency> ', text)\n",
    "    text = re_curr_4.sub(' <currency> ', text)\n",
    "    text = re_curr_3.sub(' <currency> ', text)\n",
    "    text = re_numb_1.sub(' <number> ', text)\n",
    "    \n",
    "    text = re.sub('[.]+\\s*[.]+', '. ', text)\n",
    "    text = re.sub('\\n', ' ', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RESTART:\n",
    "    with open(os.path.join(DATA_DIR, 'story_processed.txt'), 'w'):\n",
    "        None\n",
    "        \n",
    "try:\n",
    "    prev_story_log = open(os.path.join(DATA_DIR, 'story_processed.txt'), 'r').readlines()\n",
    "    prev_story_log = [re.sub(r'\\n', '', sl) for sl in prev_story_log]\n",
    "except:\n",
    "    prev_story_log = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "process cnn stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3307b192f6384f98941fa7a5ccc8b45e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=92579), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cnn_stories = os.listdir(os.path.join(DATA_DIR, 'cnn'))\n",
    "\n",
    "stories = []\n",
    "highlights = []\n",
    "\n",
    "with open(os.path.join(DATA_DIR, 'story_processed.txt'), 'a') as story_log:\n",
    "    for story_fp in tqdm_notebook(cnn_stories):\n",
    "        full_story_fp = os.path.join(DATA_DIR, 'cnn', story_fp)\n",
    "        \n",
    "        if full_story_fp not in prev_story_log:        \n",
    "            story_log.write(full_story_fp+'\\n')\n",
    "            text = open(full_story_fp, 'r', encoding='utf8').readlines()    \n",
    "\n",
    "            story = []\n",
    "            highlight = []\n",
    "            mode = 'story'\n",
    "\n",
    "            # extract text\n",
    "            for t in text:\n",
    "                if '@highlight' in t:\n",
    "                    mode = 'highlight'\n",
    "\n",
    "                if mode == 'highlight' and '@highlight' not in t and t != '\\n':\n",
    "                    highlight.append(t)\n",
    "\n",
    "                elif mode == 'story' and t != '\\n':\n",
    "                    story.append(t)\n",
    "\n",
    "            # combine text\n",
    "            story = '. '.join(story)    \n",
    "            story = clean_text(story)\n",
    "\n",
    "            highlight = '. '.join(highlight)\n",
    "            highlight = clean_text(highlight)    \n",
    "\n",
    "            # tokenize\n",
    "            story = story.lower().split()\n",
    "            highlight = highlight.lower().split()\n",
    "\n",
    "            story = [''.join([w for w in re.findall('<[a-z]+>|[a-z]+', s) if len(w) != 0]) for s in story]\n",
    "            highlight = [''.join([w for w in re.findall('<[a-z]+>|[a-z]+', s) if len(w) != 0]) for s in highlight]\n",
    "            highlight = ['<sos>'] + highlight + ['<eos>']\n",
    "\n",
    "            story = [s for s in story if s != '']\n",
    "            highlight = [s for s in highlight if s != '']\n",
    "\n",
    "            stories.append(story)\n",
    "            highlights.append(highlight)\n",
    "\n",
    "            if len(stories) % BATCH_SIZE == 0:\n",
    "                filename = len(os.listdir(SAVE_PKL_DIR)) + 1\n",
    "                filename = 'story_highlight_{}.pkl'.format(filename)        \n",
    "                obj = [stories, highlights]\n",
    "\n",
    "                pickle.dump(obj, open(os.path.join(SAVE_PKL_DIR, filename), 'wb'))\n",
    "\n",
    "                stories = []\n",
    "                highlights = []\n",
    "\n",
    "if len(stories) != 0:\n",
    "    filename = len(os.listdir(SAVE_PKL_DIR)) + 1\n",
    "    filename = 'story_highlight_{}.pkl'.format(filename)        \n",
    "    obj = [stories, highlights]\n",
    "\n",
    "    pickle.dump(obj, open(os.path.join(SAVE_PKL_DIR, filename), 'wb'))\n",
    "\n",
    "    stories = []\n",
    "    highlights = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "process dailymail stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87287d8fc0334d23a07828c08566975d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=219506), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dailymail_stories = os.listdir(os.path.join(DATA_DIR, 'dailymail'))\n",
    "\n",
    "stories = []\n",
    "highlights = []\n",
    "\n",
    "with open(os.path.join(DATA_DIR, 'story_processed.txt'), 'a') as story_log:\n",
    "    for story_fp in tqdm_notebook(dailymail_stories):        \n",
    "        full_story_fp = os.path.join(DATA_DIR, 'dailymail', story_fp)\n",
    "        \n",
    "        if full_story_fp not in prev_story_log:        \n",
    "            story_log.write(full_story_fp+'\\n')\n",
    "            text = open(full_story_fp, 'r', encoding='utf8').readlines()\n",
    "\n",
    "            story = []\n",
    "            highlight = []\n",
    "            mode = 'story'\n",
    "\n",
    "            # extract text\n",
    "            for t in text:\n",
    "                if '@highlight' in t:\n",
    "                    mode = 'highlight'\n",
    "\n",
    "                if mode == 'highlight' and '@highlight' not in t and t != '\\n':\n",
    "                    highlight.append(t)\n",
    "\n",
    "                elif mode == 'story' and t != '\\n':\n",
    "                    story.append(t)\n",
    "\n",
    "            # combine text\n",
    "            story = '. '.join(story)    \n",
    "            story = clean_text(story)\n",
    "\n",
    "            highlight = '. '.join(highlight)\n",
    "            highlight = clean_text(highlight)    \n",
    "\n",
    "            # tokenize\n",
    "            story = story.lower().split()\n",
    "            highlight = highlight.lower().split()\n",
    "\n",
    "            story = [''.join([w for w in re.findall('<[a-z]+>|[a-z]+', s) if len(w) != 0]) for s in story]\n",
    "            highlight = [''.join([w for w in re.findall('<[a-z]+>|[a-z]+', s) if len(w) != 0]) for s in highlight]\n",
    "            highlight = ['<sos>'] + highlight + ['<eos>']\n",
    "\n",
    "            story = [s for s in story if s != '']\n",
    "            highlight = [s for s in highlight if s != '']\n",
    "\n",
    "            stories.append(story)\n",
    "            highlights.append(highlight)\n",
    "\n",
    "            if len(stories) % BATCH_SIZE == 0:\n",
    "                filename = len(os.listdir(SAVE_PKL_DIR)) + 1\n",
    "                filename = 'story_highlight_{}.pkl'.format(filename)        \n",
    "                obj = [stories, highlights]\n",
    "\n",
    "                pickle.dump(obj, open(os.path.join(SAVE_PKL_DIR, filename), 'wb'))\n",
    "\n",
    "                stories = []\n",
    "                highlights = []\n",
    "\n",
    "if len(stories) != 0:\n",
    "    filename = len(os.listdir(SAVE_PKL_DIR)) + 1\n",
    "    filename = 'story_highlight_{}.pkl'.format(filename)        \n",
    "    obj = [stories, highlights]\n",
    "\n",
    "    pickle.dump(obj, open(os.path.join(SAVE_PKL_DIR, filename), 'wb'))\n",
    "\n",
    "    stories = []\n",
    "    highlights = []"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
